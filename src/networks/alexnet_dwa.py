import sys
import torch
import numpy as np

import utils

class Conv2d_dwa(torch.nn.Module):
    '''Conv2d Layer that implements dynamic weight mask.'''
    # test this: https://discuss.pytorch.org/t/custom-convolution-layer/45979/5
    # check if mask can be returned as well!
    # TODO: implement combination approach?
    pass

class Linear_dwa(torch.nn.Module):
    '''Dense Layer that implements dynamic weight mask.'''
    pass

class Net(torch.nn.Module):
    '''Alex-Net inspired variant of the dynamic weight allocation network.

    Args:
        inputsize (tuple): Input size of the images in format (channels, size, size)
        taskcla (list): List of the classes per task
        use_processor (bool): Defines if the processor should be used (otherwise embedding is one-hot task vector)
        emb_size (int): Size of the context embedding that is generated by the embedding processor (None = number of tasks)
        use_stem (int): Defines the number of stem layers should be executed first before passing to the context processor (None = use input)
        use_concat (bool): Defines if all previous layers should be concatenated for the context processor
        use_combination (bool): Defines if the attention mask should be computed through a combination of layers to save weights
    '''

    def __init__(self,inputsize,taskcla, use_processor=True, processor_feats=(10, 32), emb_size=None, use_stem=None, use_concat=False, use_combination=True):
        super(Net,self).__init__()

        # safty checks
        if use_stem >= 5:
            raise ValueError("The value of use_stem ({}) is larger than the number of layers (5)!".format(use_stem))

        # set internal values
        ncha, size, _ = inputsize
        self.taskcla=taskcla
        self.use_stem = use_stem
        self.use_processor = use_processor
        self.is_linear_processor = use_stem > 2
        self.use_combination = use_combination
        self.use_concat = use_concat
        self.emb_size = len(taskcla) if emb_size is None or use_processor is False else emb_size

        # create all relevant convolutions (either native as stem or dwa masked)
        self.processor_size = inputsize
        self.c1,s,self.processor_size = self._create_conv(ncha, 64, size//8, size, 0, use_stem, self.processor_size)
        self.c2,s,self.processor_size = self._create_conv(64, 128, size//10, s, 1, use_stem, self.processor_size)
        self.c3,s,self.processor_size = self._create_conv(128, 256, 2, s, 2, use_stem, self.processor_size)
        self.smid=s
        self.maxpool=torch.nn.MaxPool2d(2)
        self.relu=torch.nn.ReLU()

        self.drop1=torch.nn.Dropout(0.2)
        self.drop2=torch.nn.Dropout(0.5)
        self.fc1,self.processor_size = self._create_linear(256*self.smid*self.smid,2048, 3, use_stem, self.processor_size)
        self.fc2,self.processor_size = self._create_linear(2048,2048, 4, use_stem, self.processor_size)

        # generate task processor
        # all context processor stuff should start with 'p'
        self.cin = None
        if use_processor is True:
            # params
            f_bn, f_out = processor_feats
            
            # adjust layers if input from FC
            if self.is_linear_processor:
                self.pfc1 = torch.nn.Linear(self.processor_size, f_bn)
                self.pfc2 = torch.nn.Linear(f_bn, f_out)
                self.pfc3 = torch.nn.Linear(f_out, self.emb_size)
            else:
                self.pc1 = torch.nn.Conv2d(self.processor_size[0], f_bn, (1,1), (1,1), 0)
                self.pc2 = torch.nn.Conv2d(f_bn, f_out, (3,3), (2,2), 2)
                pin = self.cin//2
                self.pfc1 = torch.nn.Linear(cin*cin*f_out, self.emb_size)

        # generate all possible heads (and put them in list - list is needed for torch to properly detect layers)
        self.last=torch.nn.ModuleList()
        for t,n in self.taskcla:
            self.last.append(torch.nn.Linear(2048,n))

        # gates for this approach
        self.gate=torch.nn.Sigmoid()

        # All embedding stuff should start with 'e'
        # NOTE: the size of the outputs is oriented at the kernel size
        # input is the task id (one-hot) for each embedding layer
        self.ec1=torch.nn.Embedding(len(self.taskcla),64)
        self.ec2=torch.nn.Embedding(len(self.taskcla),128)
        self.ec3=torch.nn.Embedding(len(self.taskcla),256)
        self.efc1=torch.nn.Embedding(len(self.taskcla),2048)
        self.efc2=torch.nn.Embedding(len(self.taskcla),2048)
        """ (e.g., used in the compression experiments)
        lo,hi=0,2
        self.ec1.weight.data.uniform_(lo,hi)
        self.ec2.weight.data.uniform_(lo,hi)
        self.ec3.weight.data.uniform_(lo,hi)
        self.efc1.weight.data.uniform_(lo,hi)
        self.efc2.weight.data.uniform_(lo,hi)
        #"""

        return
    
    def _create_conv(self, fin, fout, ksize, s, pos, stem, psize):
        '''Decides whether to create a regular or weight masked convolution.'''
        # compute new kernel size
        s=utils.compute_conv_output_size(s,ksize)
        s=s//2

        # update conv
        if pos <= stem:
            conv=torch.nn.Conv2d(fin,fout,kernel_size=ksize)
            psize = (fout, s, s)
            return conv,s,psize
        else:
            # TODO: return other items as module list (inspect kernel size?)
            conv=Conv2d_dwa(fin,fout,kernel_size=ksize, comb=self.use_combination)
            return conv,s,psize
    
    def _create_linear(self, fin, fout, pos, stem, psize):
        '''Decides whether to create a regular or weight masked linear layer.'''
        if pos <= stem:
            fc = torch.nn.Linear(fin,fout)
            psize = (fout,)
            return fc, psize
        else:
            # TODO: return other items as ModuleList (calc weight size)
            fc = Linear_dwa(fin,fout, comb=self.use_combination)
            return fc, psize

    def forward(self,t,x):
        '''Computes the forward pass of the network.

        Args:
            t (int): Current task
            x (float): input images
        '''
        # define input for the context processor
        p = x
        # TODO: iterate through stem
        # TODO: check for concat (adjust size of input?)
        p = torch.cat(p, h) if self.use_concat else h
        # generate the embedding based on input
        emb = self.processor(x) if self.use_processor else t

        # compute the kernel masks
        masks=self.mask(emb)
        gc1,gc2,gc3,gfc1,gfc2=masks

        layer_list = [(self.c1, self.drop1), (self.c2, self.drop1), (self.c3, self.drop2)]

        # Gated (apply gates after each layer to control gradient flow)
        h=self.maxpool(self.drop1(self.relu(self.c1(x))))
        h=h*gc1.view(1,-1,1,1).expand_as(h)
        h=self.maxpool(self.drop1(self.relu(self.c2(h))))
        h=h*gc2.view(1,-1,1,1).expand_as(h)
        h=self.maxpool(self.drop2(self.relu(self.c3(h))))
        h=h*gc3.view(1,-1,1,1).expand_as(h)
        h=h.view(x.size(0),-1)
        h=self.drop2(self.relu(self.fc1(h)))
        h=h*gfc1.expand_as(h)
        h=self.drop2(self.relu(self.fc2(h)))
        h=h*gfc2.expand_as(h)

        # generate list of outputs from each head
        y=[]
        for i,_ in self.taskcla:
            y.append(self.last[i](h))
        return y,emb,masks
    
    def processor(self, x):
        # compute the embedding from processor
        if self.is_linear_processor:
            emb = x.view((x.size(0), -1))
            emb = self.relu(self.pfc1(emb))
            emb = self.relu(self.pfc2(emb))
            emb = self.gate(self.pfc3(emb))
        else:
            emb = self.relu(self.pc1(x))
            emb = self.relu(self.pc2(emb))
            emb = emb.view((x.size(0), -1))
            emb = self.gate(self.pfc1(emb))
        return emb

    def mask(self,emb):
        # TODO: adjust the kernel size?
        mc1 = self.gate(self.ec1(emb).view(emb.size(0), 3, 3, 32))
        gc1=self.gate(s*self.ec1(t))
        gc2=self.gate(s*self.ec2(t))
        gc3=self.gate(s*self.ec3(t))
        gfc1=self.gate(s*self.efc1(t))
        gfc2=self.gate(s*self.efc2(t))
        return [gc1,gc2,gc3,gfc1,gfc2]

    def get_view_for(self,n,masks):
        gc1,gc2,gc3,gfc1,gfc2=masks
        if n=='fc1.weight':
            post=gfc1.data.view(-1,1).expand_as(self.fc1.weight)
            pre=gc3.data.view(-1,1,1).expand((self.ec3.weight.size(1),self.smid,self.smid)).contiguous().view(1,-1).expand_as(self.fc1.weight)
            return torch.min(post,pre)
        elif n=='fc1.bias':
            return gfc1.data.view(-1)
        elif n=='fc2.weight':
            post=gfc2.data.view(-1,1).expand_as(self.fc2.weight)
            pre=gfc1.data.view(1,-1).expand_as(self.fc2.weight)
            return torch.min(post,pre)
        elif n=='fc2.bias':
            return gfc2.data.view(-1)
        elif n=='c1.weight':
            return gc1.data.view(-1,1,1,1).expand_as(self.c1.weight)
        elif n=='c1.bias':
            return gc1.data.view(-1)
        elif n=='c2.weight':
            post=gc2.data.view(-1,1,1,1).expand_as(self.c2.weight)
            pre=gc1.data.view(1,-1,1,1).expand_as(self.c2.weight)
            return torch.min(post,pre)
        elif n=='c2.bias':
            return gc2.data.view(-1)
        elif n=='c3.weight':
            post=gc3.data.view(-1,1,1,1).expand_as(self.c3.weight)
            pre=gc2.data.view(1,-1,1,1).expand_as(self.c3.weight)
            return torch.min(post,pre)
        elif n=='c3.bias':
            return gc3.data.view(-1)
        return None
